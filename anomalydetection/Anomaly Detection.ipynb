{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Anomaly Detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkA8lqnniweVmVTOQssurG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zYCL7CE0VxU1","colab_type":"text"},"source":["# Anomaly Detection\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vg1snqJ5VwEH","colab_type":"text"},"source":["이상치 탐지(Annomaly Detection) 는 원래 통계학의 과업 중 하나 입니다.\n","\n","정상 범주를 넘어선 관측값에 대한 판정이 이상치 탐지에 해당하는데, 통계적 품질관리등의 분야에서 활용됩니다."]},{"cell_type":"markdown","metadata":{"id":"kw-wNwlUZ-FV","colab_type":"text"},"source":["## 영상 처리와 Anomaly Detection\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H4TXHYO2oq0Z","colab_type":"text"},"source":["이번 예제의 목적은 이상한 것을 찾아내는 것입니다.\n","\n","이미 알고 있는 데이터와는 크게 다른 모습을 보이는 것을 찾는 것이죠. 이에 대한 전통적인 방법은 분산치를 이용하는 방법인데, 딥러닝에서도 크게 다르지 않습니다. MSE를 통해서 찾는 방법을 사용합니다.\n","\n","이번 예제에서 사용할 방법은 AutoEncoder입니다. AutoEncoder는 대표적인 비지도학습의 한 방법입니다.\n","\n","신경망의 노드를 갈수록 줄여 자료를 압축한 뒤, 그 압축된 자료를 풀어 원본을 작업하는 과정을 거치는 것으로 학습하게 됩니다.\n","\n","실제 학습에서는 2개의 라벨을 통해 학습을 진행합니다. 1에 해당하는 값을 정상치, 3에 해당하는 값을 이상치로 둡니다. 이상치 탐지도 결국에는 이진 분류문제이므로 이런 방식으로 처리한 것 같군요."]},{"cell_type":"code","metadata":{"id":"0c4cO_hynJSd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596449482460,"user_tz":-540,"elapsed":3387,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}}},"source":["from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","import numpy as np\n","# from pyimagesearch.convautoencoder import ConvAutoencoder\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.datasets import mnist\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# import argparse\n","import random\n","import pickle\n","import cv2\n","import matplotlib\n","matplotlib.use(\"Agg\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9ItX3KcnTrJ","colab_type":"text"},"source":["일단 필요한 패키지들은 위와 같습니다."]},{"cell_type":"markdown","metadata":{"id":"cMwEnSueZ7iP","colab_type":"text"},"source":["## 모델 설정"]},{"cell_type":"markdown","metadata":{"id":"C_y5nmaCmMC5","colab_type":"text"},"source":["일반적인 Autoencoder는 Affine(Dense) 만을 사용한 일반적인 계층이지만, 여기서는 ConvNet을 사용하여 특질 추출 과정을 사용합니다.\n","\n","\n","인코더 :\n","\n","ConvNet을 통과해 특질 추출을 마친 신호는 Dense를 거쳐 잠재의미 신호로 변환 되는데, 이 잠재의미 신호는 반드시 이전 입력값보다 노드의 수가 적어야 합니다.\n","\n","디코더 :\n","\n","여타 다른 AutoEncoder와 마찬가지로 적은 차원의 잠재의미 신호를 점차 불려나가 원본을 복원하는 형태로 진행합니다.\n","\n","이 때, 원본이 ConvNet을 통과하였기 때문에 Conv Transpose를 사용하여 역으로 입력값을 증가시키는 방법을 사용합니다.(저도 잘 몰라서 수학적 설명은 생략합니다.)\n","\n","오토 인코더 :\n","\n","이렇게 만든 인코더와 디코더를 이어 붙으면 그것이 곧 오토인코더 입니다. 실제 구현체에서는 케라스 함수형 API를 사용해 인코더의 입력과 디코더의 출력을 연결했습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"a71bCORkZ5dq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596449482462,"user_tz":-540,"elapsed":2869,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}}},"source":["# import the necessary packages\n","\n","class ConvAutoencoder:\n","\t@staticmethod\n","\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n","\t\t# initialize the input shape to be \"channels last\" along with\n","\t\t# the channels dimension itself\n","\t\t# channels dimension itself\n","\t\tinputShape = (height, width, depth)\n","\t\tchanDim = -1\n","\n","\t\t# define the input to the encoder\n","\t\tinputs = Input(shape=inputShape)\n","\t\tx = inputs\n","\n","\t\t# loop over the number of filters\n","\t\tfor f in filters:\n","\t\t\t# apply a CONV => RELU => BN operation\n","\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n","\t\t\tx = LeakyReLU(alpha=0.2)(x)\n","\t\t\tx = BatchNormalization(axis=chanDim)(x)\n","\n","\t\t# flatten the network and then construct our latent vector\n","\t\tvolumeSize = K.int_shape(x)\n","\t\tx = Flatten()(x)\n","\t\tlatent = Dense(latentDim)(x)\n","\n","\t\t# build the encoder model\n","\t\tencoder = Model(inputs, latent, name=\"encoder\")\n","\n","\t\t# start building the decoder model which will accept the\n","\t\t# output of the encoder as its inputs\n","\t\tlatentInputs = Input(shape=(latentDim,))\n","\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n","\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n","\n","\t\t# loop over our number of filters again, but this time in\n","\t\t# reverse order\n","\t\tfor f in filters[::-1]:\n","\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n","\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n","\t\t\t\tpadding=\"same\")(x)\n","\t\t\tx = LeakyReLU(alpha=0.2)(x)\n","\t\t\tx = BatchNormalization(axis=chanDim)(x)\n","\n","\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n","\t\t# original depth of the image\n","\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n","\t\toutputs = Activation(\"sigmoid\")(x)\n","\n","\t\t# build the decoder model\n","\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n","\n","\t\t# our autoencoder is the encoder + decoder\n","\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n","\t\t\tname=\"autoencoder\")\n","\n","\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n","\t\treturn (encoder, decoder, autoencoder)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"csVMRIx9mjQg","colab_type":"text"},"source":["## 데이터 셋 빌드"]},{"cell_type":"markdown","metadata":{"id":"djSrR_ZQmm00","colab_type":"text"},"source":["데이터셋으로는 간단한 mnist data를 사용합니다. 아래 함수는 데이터 셋을 랜덤하게 섞고 찾고자 하는 라벨만 가져오는 함수입니다.\n","\n","※ np.where 는 조건에 맞는 idx 값을 찾아줍니다.\n","\n","※ random.shuffle, np.shuffle 모두 반환값이 없습니다.\n","\n","※ np.vstack 은 행을 추가해줍니다. np.concat(axis=0) 으로도 같은 효과를 봅니다.\n"]},{"cell_type":"code","metadata":{"id":"fChS5gMkmKiJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596449482462,"user_tz":-540,"elapsed":2509,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}}},"source":["def build_unsupervised_dataset(data, labels, validLabel=1,\n","\tanomalyLabel=3, contam=0.01, seed=42):\n","\t# grab all indexes of the supplied class label that are *truly*\n","\t# that particular label, then grab the indexes of the image\n","\t# labels that will serve as our \"anomalies\"\n","\tvalidIdxs = np.where(labels == validLabel)[0]\n","\tanomalyIdxs = np.where(labels == anomalyLabel)[0]\n","\n","\t# randomly shuffle both sets of indexes\n","\trandom.shuffle(validIdxs)\n","\trandom.shuffle(anomalyIdxs)\n","\n","\t# compute the total number of anomaly data points to select\n","\ti = int(len(validIdxs) * contam)\n","\tanomalyIdxs = anomalyIdxs[:i]\n","\n","\t# use NumPy array indexing to extract both the valid images and\n","\t# \"anomlay\" images\n","\tvalidImages = data[validIdxs]\n","\tanomalyImages = data[anomalyIdxs]\n","\n","\t# stack the valid images and anomaly images together to form a\n","\t# single data matrix and then shuffle the rows\n","\timages = np.vstack([validImages, anomalyImages])\n","\tnp.random.seed(seed)\n","\tnp.random.shuffle(images)\n","\n","\t# return the set of images\n","\treturn images"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH5CYTfwmnjO","colab_type":"text"},"source":["## 예측 시각화"]},{"cell_type":"markdown","metadata":{"id":"iKfGzf6CxpO7","colab_type":"text"},"source":["예측 결과를 원본과 묶어서 시각적으로 표현할 수 있게 만들어주는 함수 입니다.\n","\n","흑백 사진을 기준으로 하므로, 픽셀 하나하나에 0~255 까지의 명도 값이 들어가게 됩니다.\n","\n","※ np.hstack 은 열을 추가해줍니다. np.concat(axis=1) 으로도 같은 효과를 봅니다."]},{"cell_type":"code","metadata":{"id":"22d9E9S4mrdE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596449482462,"user_tz":-540,"elapsed":1774,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}}},"source":["def visualize_predictions(decoded, gt, samples=10):\n","\t# initialize our list of output images\n","\toutputs = None\n","\n","\t# loop over our number of output samples\n","\tfor i in range(0, samples):\n","\t\t# grab the original image and reconstructed image\n","\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n","\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n","\n","\t\t# stack the original and reconstructed image side-by-side\n","\t\toutput = np.hstack([original, recon])\n","\n","\t\t# if the outputs array is empty, initialize it as the current\n","\t\t# side-by-side image display\n","\t\tif outputs is None:\n","\t\t\toutputs = output\n","\n","\t\t# otherwise, vertically stack the outputs\n","\t\telse:\n","\t\t\toutputs = np.vstack([outputs, output])\n","\n","\t# return the output images\n","\treturn outputs"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhbO0oT3o4bX","colab_type":"text"},"source":["## 매개변수 입력"]},{"cell_type":"markdown","metadata":{"id":"nvtlSKAxyO_N","colab_type":"text"},"source":["항상 그렇듯 매개변수를 입력해 줄 수가 없으니 해당 아규먼트들을 분석해 일일이 적어줄 필요가 있습니다."]},{"cell_type":"code","metadata":{"id":"RUVv0qdCo5g0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1596449483014,"user_tz":-540,"elapsed":1066,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}},"outputId":"f81c180a-dc56-437e-f3c3-ed9baaca5419"},"source":["# import argparse\n","# # construct the argument parse and parse the arguments\n","# ap = argparse.ArgumentParser()\n","# ap.add_argument(\"-d\", \"--dataset\", type=str, required=True,\n","# \thelp=\"path to output dataset file\")\n","# ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n","# \thelp=\"path to output trained autoencoder\")\n","# ap.add_argument(\"-v\", \"--vis\", type=str, default=\"recon_vis.png\",\n","# \thelp=\"path to output reconstruction visualization file\")\n","# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n","# \thelp=\"path to output plot file\")\n","# args = vars(ap.parse_args())\n","\n","\n","# 경로를 제 구글 드라이브에 주기위한 코드입니다.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/AnomalyDetection\n","\n","# 다행히 모든 아규먼트가 경로를 지정할 뿐입니다.\n","# 솔직히 귀찮아서 같은 폴더에 몰아두기로 합니다.\n","dataset_path = \"dataset\"\n","model_path = \"model\"\n","vis_path = \"vis.png\"\n","plot_path = \"plot.png\"\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Colab Notebooks/AnomalyDetection\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7V-p-XIHzqNa","colab_type":"text"},"source":["## 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"5lwxSODnzt-B","colab_type":"text"},"source":["데이터는 mnist인데, 따로 준비할 필요는 없고 다운로드 받으면 됩니다.\n","\n","그 후 아주 약간의 전처리를 가해줍니다.\n","\n","위에서 만들었던 함수를 호출해서 데이터 셋을 처리해주고, \n","\n","입력에 알맞게 차원을 하나 늘려준뒤 (channel 차원을 맞춰줍니다.),\n","\n","값의 범위를 0~1로 조정합니다.\n","\n","그리고 적당히 나눠줍니다."]},{"cell_type":"code","metadata":{"id":"IULWudtc0l2a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1596449906387,"user_tz":-540,"elapsed":1219,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}},"outputId":"1197b72e-fd5a-49ed-ff56-d7aab96ed0dd"},"source":["# load the MNIST dataset\n","print(\"[INFO] loading MNIST dataset...\")\n","((trainX, trainY), (testX, testY)) = mnist.load_data()\n","\n","# build our unsupervised dataset of images with a small amount of\n","# contamination (i.e., anomalies) added into it\n","print(\"[INFO] creating unsupervised dataset...\")\n","images = build_unsupervised_dataset(trainX, trainY, validLabel=1,\n","\tanomalyLabel=3, contam=0.01)\n","\n","# add a channel dimension to every image in the dataset, then scale\n","# the pixel intensities to the range [0, 1]\n","images = np.expand_dims(images, axis=-1) # axis = 0 에 추가되는 것 같습니다.\n","images = images.astype(\"float32\") / 255.0\n","\n","# construct the training and testing split\n","(trainX, testX) = train_test_split(images, test_size=0.2,\n","\trandom_state=42)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[INFO] loading MNIST dataset...\n","[INFO] creating unsupervised dataset...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-z7X8qUv1Gyd","colab_type":"text"},"source":["## 파라미터 설정 및 학습"]},{"cell_type":"markdown","metadata":{"id":"Ejs3wY5S1Jfq","colab_type":"text"},"source":["Optimizer 로는 Adam을 사용합니다.\n","\n","loss는 처음에 밝혔는 mse를 사용합니다.\n","\n","아래 코드에서는 autoencoder만 사용했지만, encoder의 경우, 학습을 막고 전이학습을 위한 용도로 사용할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"2f-xqgCPmvYl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":923},"executionInfo":{"status":"ok","timestamp":1596449784619,"user_tz":-540,"elapsed":262834,"user":{"displayName":"전현구","photoUrl":"","userId":"07243786492781807594"}},"outputId":"5c308753-61f8-4341-ee1a-e50ffec92f3e"},"source":["# initialize the number of epochs to train for, initial learning rate,\n","# and batch size\n","EPOCHS = 20\n","INIT_LR = 1e-3\n","BS = 32\n","\n","# construct our convolutional autoencoder\n","print(\"[INFO] building autoencoder...\")\n","(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n","opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n","autoencoder.compile(loss=\"mse\", optimizer=opt)\n","\n","# train the convolutional autoencoder\n","H = autoencoder.fit(\n","\ttrainX, trainX,\n","\tvalidation_data=(testX, testX),\n","\tepochs=EPOCHS,\n","\tbatch_size=BS)\n","\n","# use the convolutional autoencoder to make predictions on the\n","# testing images, construct the visualization, and then save it\n","# to disk\n","print(\"[INFO] making predictions...\")\n","decoded = autoencoder.predict(testX)\n","vis = visualize_predictions(decoded, testX)\n","cv2.imwrite(vis_path, vis)\n","\n","# construct a plot that plots and saves the training history\n","N = np.arange(0, EPOCHS)\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(plot_path)\n","\n","# serialize the image data to disk\n","print(\"[INFO] saving image data...\")\n","f = open(dataset_path, \"wb\")\n","f.write(pickle.dumps(images))\n","f.close()\n","\n","# serialize the autoencoder model to disk\n","print(\"[INFO] saving autoencoder...\")\n","autoencoder.save(model_path, save_format=\"h5\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[INFO] loading MNIST dataset...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","[INFO] creating unsupervised dataset...\n","[INFO] building autoencoder...\n","Epoch 1/20\n","171/171 [==============================] - 13s 75ms/step - loss: 0.0418 - val_loss: 0.0383\n","Epoch 2/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0117 - val_loss: 0.0214\n","Epoch 3/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0044 - val_loss: 0.0088\n","Epoch 4/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0032 - val_loss: 0.0035\n","Epoch 5/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0029 - val_loss: 0.0028\n","Epoch 6/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0026 - val_loss: 0.0029\n","Epoch 7/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0026 - val_loss: 0.0026\n","Epoch 8/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0023 - val_loss: 0.0025\n","Epoch 9/20\n","171/171 [==============================] - 13s 74ms/step - loss: 0.0022 - val_loss: 0.0025\n","Epoch 10/20\n","171/171 [==============================] - 13s 75ms/step - loss: 0.0021 - val_loss: 0.0024\n","Epoch 11/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0020 - val_loss: 0.0023\n","Epoch 12/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0020 - val_loss: 0.0027\n","Epoch 13/20\n","171/171 [==============================] - 13s 75ms/step - loss: 0.0020 - val_loss: 0.0024\n","Epoch 14/20\n","171/171 [==============================] - 13s 75ms/step - loss: 0.0019 - val_loss: 0.0023\n","Epoch 15/20\n","171/171 [==============================] - 13s 75ms/step - loss: 0.0018 - val_loss: 0.0024\n","Epoch 16/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0018 - val_loss: 0.0022\n","Epoch 17/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0017 - val_loss: 0.0022\n","Epoch 18/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0017 - val_loss: 0.0022\n","Epoch 19/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0016 - val_loss: 0.0022\n","Epoch 20/20\n","171/171 [==============================] - 13s 76ms/step - loss: 0.0016 - val_loss: 0.0021\n","[INFO] making predictions...\n","[INFO] saving image data...\n","[INFO] saving autoencoder...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fSL60ohAzn41","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}