{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_classificaiton_with_fashionnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYsTlNUTrsc1",
        "colab_type": "text"
      },
      "source": [
        "code from https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZVX3l-EHDF",
        "colab_type": "text"
      },
      "source": [
        "구글 드라이브가 연결되어 있고, 구글 드라이브의 tmp 폴더 안에 fashion_dataset.zip, fashion_examples.zip이 있을 때,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbGGPIctHjwl",
        "colab_type": "code",
        "outputId": "17625665-d75a-469a-be1c-7df4a0fa000e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        }
      },
      "source": [
        "!ls -al \"/content/drive/My Drive/tmp/\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4140519\n",
            "drwx------ 2 root root      4096 Jun 12  2019  부산카톨릭대_교재\n",
            "-rw------- 1 root root 210504772 Apr 25 00:42  1000명_croped_images.zip\n",
            "-rw------- 1 root root    957336 Dec 27 04:43  나혼자만레벨업+1-243외전21,후일담6_001.txt\n",
            "-rw------- 1 root root    957336 Dec 27 04:43  나혼자만레벨업+1-243외전21,후일담6_002.txt\n",
            "-rw------- 1 root root    957335 Dec 27 04:43  나혼자만레벨업+1-243외전21,후일담6_003.txt\n",
            "drwx------ 2 root root      4096 Jun 18  2019  201906_중앙학원_과제신청\n",
            "-rw------- 1 root root      3478 May  8 06:23  20200505_새로추가와_비적절제거_label.zip\n",
            "-rw------- 1 root root 300574287 May  6 22:49  20200505_새로추가와_비적절제거.zip\n",
            "-rw------- 1 root root     33910 Sep 19  2019  6차.ipynb\n",
            "-rw------- 1 root root     16007 Sep 19  2019  7회차.ipynb\n",
            "-rw------- 1 root root       151 May  8  2019  AWS_인터뷰_응답메일.gdoc\n",
            "-rw------- 1 root root    301774 May  8  2019  AWS_인터뷰_응답메일.pdf\n",
            "-rw------- 1 root root 188909928 Apr 27 16:19  best_d_model.h5\n",
            "-rw------- 1 root root     26134 Jun  7  2017  create_account1.PNG\n",
            "-rw------- 1 root root     12769 Jun  7  2017  create_account2.PNG\n",
            "-rw------- 1 root root     32876 Jun  7  2017  create_account3.PNG\n",
            "-rw------- 1 root root     15823 Jun  7  2017  create_account4.PNG\n",
            "-rw------- 1 root root     35696 Jun  7  2017  create_account5.PNG\n",
            "-rw------- 1 root root     14999 Jun  7  2017  create_account6.PNG\n",
            "-rw------- 1 root root  17041583 Apr 26 14:12  crop_images.tar.gz\n",
            "-rw------- 1 root root  17144347 Apr 23 11:08  cropped_image.zip\n",
            "-rw------- 1 root root  55954102 Apr 12  2019  deep_learning.201901.pptx\n",
            "-rw------- 1 root root 124028479 Apr  1 05:47  eye.zip\n",
            "-rw------- 1 root root 537686706 May 29 05:41  fashion_dataset.zip\n",
            "-rw------- 1 root root    215899 May 29 05:41  fashion_examples.zip\n",
            "-rw------- 1 root root 264478704 May 10 06:43  glaucoma.h5\n",
            "-rw------- 1 root root 124000887 May 10 05:38  glaucoma.zip\n",
            "-rw------- 1 root root 209299198 Apr 15  2019  graph_opt.pb\n",
            "-rw------- 1 root root      5444 Apr 13 23:16  HE_200410_cNN_postmeno_noNA2.csv\n",
            "drwx------ 2 root root      4096 Oct  7  2019  honeynaps\n",
            "-rw------- 1 root root   1489954 May 15  2019  Image_Classification_VGG16.ipynb\n",
            "-rw------- 1 root root 419360231 May 12 02:04  input_data_0512_all_prepared.tar.gz\n",
            "-rw------- 1 root root  56761730 May 11 15:16  input_data_0512.zip\n",
            "-rw------- 1 root root 422216601 Apr 27 14:58  input_data.tar.gz\n",
            "-rw------- 1 root root     28072 Sep 20  2019  보강_실습.ipynb\n",
            "-rw------- 1 root root      6666 Sep 23  2019  시험.ipynb\n",
            "-rw------- 1 root root   1235623 May 14  2019 '실습 환경 구성.ipynb'\n",
            "-rw------- 1 root root    440523 May 14  2019  Keras_GAN.ipynb\n",
            "-rw------- 1 root root   3790041 May 14  2019  Keras_SRResnet_20190221.ipynb\n",
            "-rw------- 1 root root   7815390 May 14  2019  Keras_VGG16.ipynb\n",
            "-rw------- 1 root root   9636587 May 14  2019  Keras_Yolov3.ipynb\n",
            "-rw------- 1 root root     63652 Aug 20  2019  mnist_dnn.ipynb.txt\n",
            "-rw------- 1 root root       757 Nov 13  2019  my_trial\n",
            "-rw------- 1 root root    280464 May 11 18:08  output.tar.gz\n",
            "-rw------- 1 root root    244273 Sep 10  2019 '등록금 내역서.pdf'\n",
            "-rw------- 1 root root 124019167 Feb  8 07:33  processed_data.zip\n",
            "-rw------- 1 root root     24464 Sep 10  2019  python_intro_20190910.ipynb\n",
            "-rw------- 1 root root    163176 Apr 11 10:50  sample.zip\n",
            "-rw------- 1 root root    645076 May 14  2019  segmentation.ipynb\n",
            "-rw------- 1 root root     34757 Aug 24  2019  titanic.zip\n",
            "-rw------- 1 root root 580085408 Apr  1 21:03  vgg_face_weights.h5\n",
            "-rw------- 1 root root 201485128 May 15  2019  VGG_ILSVRC2016_SSD_300x300_iter_440000.h5\n",
            "-rw------- 1 root root 356798720 May 11 18:08  wgan_d.h5\n",
            "-rw------- 1 root root     29213 May 10 07:57  견적산정.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Sc6P-GIQfJ",
        "colab_type": "code",
        "outputId": "35259496-2c15-45aa-ecc1-1733124ea121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!ls -al \"/content/drive/My Drive/tmp/fashion_dataset.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 537686706 May 29 05:41 '/content/drive/My Drive/tmp/fashion_dataset.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPJniJZeDqCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/tmp/fashion_dataset.zip\" ./\n",
        "!cp \"/content/drive/My Drive/tmp/fashion_examples.zip\" ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A14P6RuHmm9F",
        "colab_type": "code",
        "outputId": "d6e2cd23-2e6d-46a9-e111-93084d945fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!ls -al"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 525328\n",
            "drwxr-xr-x 1 root root      4096 May 29 06:29 .\n",
            "drwxr-xr-x 1 root root      4096 May 29 05:35 ..\n",
            "drwxr-xr-x 1 root root      4096 May 27 16:27 .config\n",
            "drwx------ 4 root root      4096 May 29 06:09 drive\n",
            "-rw-r--r-- 1 root root 537686706 May 29 06:34 fashion_dataset.zip\n",
            "-rw------- 1 root root    215899 May 29 06:34 fashion_examples.zip\n",
            "drwxr-xr-x 2 root root      4096 May 29 06:09 .ipynb_checkpoints\n",
            "drwxr-xr-x 1 root root      4096 May 27 16:27 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKij7JOWExOI",
        "colab_type": "code",
        "outputId": "248ec06c-46eb-4592-a7e9-88b49ff514aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!unzip fashion_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fashion_dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of fashion_dataset.zip or\n",
            "        fashion_dataset.zip.zip, and cannot find fashion_dataset.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg66z1-RE0ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC-6anHbnzaF",
        "colab_type": "code",
        "outputId": "6295709d-0316-4a10-a35e-a658aae29e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!ls -al\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 261140\n",
            "drwxr-xr-x 1 root root      4096 May 29 05:42 .\n",
            "drwxr-xr-x 1 root root      4096 May 29 05:35 ..\n",
            "drwxr-xr-x 1 root root      4096 May 27 16:27 .config\n",
            "-rw-r--r-- 1 root root 267386880 May 29 06:07 fashion_dataset.zip\n",
            "drwxr-xr-x 1 root root      4096 May 27 16:27 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7KTbqSYp20e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "# matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from pyimagesearch.fashionnet import FashionNet\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7EcCnKp30I",
        "colab_type": "code",
        "outputId": "235b2592-4309-4bed-9c47-cbe8df871fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# USAGE\n",
        "# python train.py --dataset dataset --model output/fashion.model \\\n",
        "#\t--categorybin output/category_lb.pickle --colorbin output/color_lb.pickle\n",
        "\n",
        "# # construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
        "# \thelp=\"path to input dataset (i.e., directory of images)\")\n",
        "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
        "# \thelp=\"path to output model\")\n",
        "# ap.add_argument(\"-l\", \"--categorybin\", required=True,\n",
        "# \thelp=\"path to output category label binarizer\")\n",
        "# ap.add_argument(\"-c\", \"--colorbin\", required=True,\n",
        "# \thelp=\"path to output color label binarizer\")\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"output\",\n",
        "# \thelp=\"base filename for generated plots\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "args = { \"dataset\":\"multi-output-classification/dataset\", \n",
        "        \"model\":\"multi-output-classification/output/fashion.model\", \n",
        "        \"categorybin\":\"multi-output-classification/output/category_lb.pickle\", \n",
        "        \"colorbin\":\"multi-output-classification/output/color_lb.pickle\", \n",
        "        \"plot\":\"multi-output-classification/output\" }\n",
        "\n",
        "\n",
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# batch size, and image dimensions\n",
        "EPOCHS = 50\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "IMAGE_DIMS = (96, 96, 3)\n",
        "\n",
        "\n",
        "# grab the image paths and randomly shuffle them\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "\n",
        "# initialize the data, clothing category labels (i.e., shirts, jeans,\n",
        "# dresses, etc.) along with the color labels (i.e., red, blue, etc.)\n",
        "data = []\n",
        "categoryLabels = []\n",
        "colorLabels = []\n",
        "\n",
        "# loop over the input images\n",
        "for imagePath in imagePaths:\n",
        "\t# load the image, pre-process it, and store it in the data list\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\timage = img_to_array(image)\n",
        "\tdata.append(image)\n",
        "\n",
        "\t# extract the clothing color and category from the path and\n",
        "\t# update the respective lists\n",
        "\t(color, cat) = imagePath.split(os.path.sep)[-2].split(\"_\")\n",
        "\tcategoryLabels.append(cat)\n",
        "\tcolorLabels.append(color)\n",
        "\n",
        "# scale the raw pixel intensities to the range [0, 1] and convert to\n",
        "# a NumPy array\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(\n",
        "\tlen(imagePaths), data.nbytes / (1024 * 1000.0)))\n",
        "\n",
        "# convert the label lists to NumPy arrays prior to binarization\n",
        "categoryLabels = np.array(categoryLabels)\n",
        "colorLabels = np.array(colorLabels)\n",
        "\n",
        "# binarize both sets of labels\n",
        "print(\"[INFO] binarizing labels...\")\n",
        "categoryLB = LabelBinarizer()\n",
        "colorLB = LabelBinarizer()\n",
        "categoryLabels = categoryLB.fit_transform(categoryLabels)\n",
        "colorLabels = colorLB.fit_transform(colorLabels)\n",
        "\n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "split = train_test_split(data, categoryLabels, colorLabels, test_size=0.2, random_state=42)\n",
        "(trainX, testX, trainCategoryY, testCategoryY, trainColorY, testColorY) = split\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] data matrix: 2521 images (544.54MB)\n",
            "[INFO] binarizing labels...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X5IjbvrqKDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "import tensorflow as tf\n",
        "\n",
        "class FashionNet:\n",
        "\t@staticmethod\n",
        "\tdef build_category_branch(inputs, numCategories,\n",
        "\t\tfinalAct=\"softmax\", chanDim=-1):\n",
        "\t\t# utilize a lambda layer to convert the 3 channel input to a\n",
        "\t\t# grayscale representation\n",
        "\t\tx = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
        "\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# (CONV => RELU) * 2 => POOL\n",
        "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# (CONV => RELU) * 2 => POOL\n",
        "\t\tx = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# define a branch of output layers for the number of different\n",
        "\t\t# clothing categories (i.e., shirts, jeans, dresses, etc.)\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(256)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization()(x)\n",
        "\t\tx = Dropout(0.5)(x)\n",
        "\t\tx = Dense(numCategories)(x)\n",
        "\t\tx = Activation(finalAct, name=\"category_output\")(x)\n",
        "\n",
        "\t\t# return the category prediction sub-network\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build_color_branch(inputs, numColors, finalAct=\"softmax\",\n",
        "\t\tchanDim=-1):\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tx = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# CONV => RELU => POOL\n",
        "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\t\tx = Dropout(0.25)(x)\n",
        "\n",
        "\t\t# define a branch of output layers for the number of different\n",
        "\t\t# colors (i.e., red, black, blue, etc.)\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(128)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = BatchNormalization()(x)\n",
        "\t\tx = Dropout(0.5)(x)\n",
        "\t\tx = Dense(numColors)(x)\n",
        "\t\tx = Activation(finalAct, name=\"color_output\")(x)\n",
        "\n",
        "\t\t# return the color prediction sub-network\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, numCategories, numColors,\n",
        "\t\tfinalAct=\"softmax\"):\n",
        "\t\t# initialize the input shape and channel dimension (this code\n",
        "\t\t# assumes you are using TensorFlow which utilizes channels\n",
        "\t\t# last ordering)\n",
        "\t\tinputShape = (height, width, 3)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# construct both the \"category\" and \"color\" sub-networks\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tcategoryBranch = FashionNet.build_category_branch(inputs,\n",
        "\t\t\tnumCategories, finalAct=finalAct, chanDim=chanDim)\n",
        "\t\tcolorBranch = FashionNet.build_color_branch(inputs,\n",
        "\t\t\tnumColors, finalAct=finalAct, chanDim=chanDim)\n",
        "\n",
        "\t\t# create the model using our input (the batch of images) and\n",
        "\t\t# two separate outputs -- one for the clothing category\n",
        "\t\t# branch and another for the color branch, respectively\n",
        "\t\tmodel = Model(\n",
        "\t\t\tinputs=inputs,\n",
        "\t\t\toutputs=[categoryBranch, colorBranch],\n",
        "\t\t\tname=\"fashionnet\")\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu1ESKRo3ih1",
        "colab_type": "code",
        "outputId": "c0fe456d-8fcc-4436-b3e6-a3b912646a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initialize our FashionNet multi-output network\n",
        "model = FashionNet.build(96, 96,\n",
        "\tnumCategories=len(categoryLB.classes_),\n",
        "\tnumColors=len(colorLB.classes_),\n",
        "\tfinalAct=\"softmax\")\n",
        "\n",
        "# define two dictionaries: one that specifies the loss method for\n",
        "# each output of the network along with a second dictionary that\n",
        "# specifies the weight per loss\n",
        "losses = {\n",
        "\t\"category_output\": \"categorical_crossentropy\",\n",
        "\t\"color_output\": \"categorical_crossentropy\",\n",
        "}\n",
        "lossWeights = {\"category_output\": 1.0, \"color_output\": 1.0}\n",
        "\n",
        "# initialize the optimizer and compile the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the network to perform multi-output classification\n",
        "H = model.fit(trainX,\n",
        "\t{\"category_output\": trainCategoryY, \"color_output\": trainColorY},\n",
        "\tvalidation_data=(testX,\n",
        "\t\t{\"category_output\": testCategoryY, \"color_output\": testColorY}),\n",
        "\tepochs=EPOCHS,\n",
        "\tverbose=1)\n",
        "\n",
        "# save the model to disk\n",
        "print(\"[INFO] serializing network...\")\n",
        "model.save(args[\"model\"])\n",
        "\n",
        "# save the category binarizer to disk\n",
        "print(\"[INFO] serializing category label binarizer...\")\n",
        "f = open(args[\"categorybin\"], \"wb\")\n",
        "f.write(pickle.dumps(categoryLB))\n",
        "f.close()\n",
        "\n",
        "# save the color binarizer to disk\n",
        "print(\"[INFO] serializing color label binarizer...\")\n",
        "f = open(args[\"colorbin\"], \"wb\")\n",
        "f.write(pickle.dumps(colorLB))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "Train on 2016 samples, validate on 505 samples\n",
            "Epoch 1/50\n",
            "2016/2016 [==============================] - 10s 5ms/step - loss: 0.9572 - category_output_loss: 0.6167 - color_output_loss: 0.3404 - category_output_accuracy: 0.8175 - color_output_accuracy: 0.8735 - val_loss: 2.8219 - val_category_output_loss: 1.6293 - val_color_output_loss: 1.1912 - val_category_output_accuracy: 0.3188 - val_color_output_accuracy: 0.4475\n",
            "Epoch 2/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.3694 - category_output_loss: 0.2550 - color_output_loss: 0.1144 - category_output_accuracy: 0.9201 - color_output_accuracy: 0.9588 - val_loss: 2.9823 - val_category_output_loss: 1.7012 - val_color_output_loss: 1.2766 - val_category_output_accuracy: 0.3644 - val_color_output_accuracy: 0.4475\n",
            "Epoch 3/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.2580 - category_output_loss: 0.1821 - color_output_loss: 0.0759 - category_output_accuracy: 0.9410 - color_output_accuracy: 0.9727 - val_loss: 2.8335 - val_category_output_loss: 1.8013 - val_color_output_loss: 1.0275 - val_category_output_accuracy: 0.5267 - val_color_output_accuracy: 0.5030\n",
            "Epoch 4/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.2347 - category_output_loss: 0.1758 - color_output_loss: 0.0589 - category_output_accuracy: 0.9325 - color_output_accuracy: 0.9812 - val_loss: 2.9556 - val_category_output_loss: 1.8396 - val_color_output_loss: 1.1124 - val_category_output_accuracy: 0.4257 - val_color_output_accuracy: 0.5089\n",
            "Epoch 5/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1911 - category_output_loss: 0.1344 - color_output_loss: 0.0567 - category_output_accuracy: 0.9544 - color_output_accuracy: 0.9826 - val_loss: 2.0185 - val_category_output_loss: 1.0419 - val_color_output_loss: 0.9762 - val_category_output_accuracy: 0.6851 - val_color_output_accuracy: 0.6178\n",
            "Epoch 6/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1931 - category_output_loss: 0.1343 - color_output_loss: 0.0588 - category_output_accuracy: 0.9529 - color_output_accuracy: 0.9826 - val_loss: 1.7383 - val_category_output_loss: 1.1319 - val_color_output_loss: 0.6139 - val_category_output_accuracy: 0.6673 - val_color_output_accuracy: 0.7663\n",
            "Epoch 7/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1620 - category_output_loss: 0.1169 - color_output_loss: 0.0451 - category_output_accuracy: 0.9593 - color_output_accuracy: 0.9821 - val_loss: 0.7885 - val_category_output_loss: 0.5558 - val_color_output_loss: 0.2324 - val_category_output_accuracy: 0.8119 - val_color_output_accuracy: 0.8970\n",
            "Epoch 8/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1541 - category_output_loss: 0.0964 - color_output_loss: 0.0577 - category_output_accuracy: 0.9633 - color_output_accuracy: 0.9782 - val_loss: 0.4715 - val_category_output_loss: 0.4168 - val_color_output_loss: 0.0553 - val_category_output_accuracy: 0.8693 - val_color_output_accuracy: 0.9822\n",
            "Epoch 9/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1143 - category_output_loss: 0.0669 - color_output_loss: 0.0474 - category_output_accuracy: 0.9752 - color_output_accuracy: 0.9836 - val_loss: 0.4709 - val_category_output_loss: 0.2753 - val_color_output_loss: 0.1949 - val_category_output_accuracy: 0.9347 - val_color_output_accuracy: 0.9287\n",
            "Epoch 10/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0775 - category_output_loss: 0.0500 - color_output_loss: 0.0275 - category_output_accuracy: 0.9826 - color_output_accuracy: 0.9921 - val_loss: 0.5711 - val_category_output_loss: 0.3358 - val_color_output_loss: 0.2329 - val_category_output_accuracy: 0.8970 - val_color_output_accuracy: 0.9267\n",
            "Epoch 11/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0887 - category_output_loss: 0.0488 - color_output_loss: 0.0399 - category_output_accuracy: 0.9836 - color_output_accuracy: 0.9861 - val_loss: 0.4980 - val_category_output_loss: 0.2897 - val_color_output_loss: 0.2116 - val_category_output_accuracy: 0.9287 - val_color_output_accuracy: 0.9287\n",
            "Epoch 12/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0778 - category_output_loss: 0.0500 - color_output_loss: 0.0278 - category_output_accuracy: 0.9821 - color_output_accuracy: 0.9906 - val_loss: 0.2656 - val_category_output_loss: 0.2313 - val_color_output_loss: 0.0345 - val_category_output_accuracy: 0.9366 - val_color_output_accuracy: 0.9881\n",
            "Epoch 13/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0812 - category_output_loss: 0.0498 - color_output_loss: 0.0313 - category_output_accuracy: 0.9826 - color_output_accuracy: 0.9886 - val_loss: 0.2781 - val_category_output_loss: 0.1604 - val_color_output_loss: 0.1178 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9624\n",
            "Epoch 14/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0813 - category_output_loss: 0.0369 - color_output_loss: 0.0444 - category_output_accuracy: 0.9866 - color_output_accuracy: 0.9861 - val_loss: 0.1849 - val_category_output_loss: 0.1288 - val_color_output_loss: 0.0539 - val_category_output_accuracy: 0.9703 - val_color_output_accuracy: 0.9762\n",
            "Epoch 15/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0649 - category_output_loss: 0.0356 - color_output_loss: 0.0293 - category_output_accuracy: 0.9886 - color_output_accuracy: 0.9916 - val_loss: 0.4174 - val_category_output_loss: 0.1806 - val_color_output_loss: 0.2364 - val_category_output_accuracy: 0.9505 - val_color_output_accuracy: 0.9347\n",
            "Epoch 16/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1664 - category_output_loss: 0.1251 - color_output_loss: 0.0414 - category_output_accuracy: 0.9593 - color_output_accuracy: 0.9866 - val_loss: 0.4416 - val_category_output_loss: 0.3992 - val_color_output_loss: 0.0429 - val_category_output_accuracy: 0.9208 - val_color_output_accuracy: 0.9822\n",
            "Epoch 17/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0853 - category_output_loss: 0.0586 - color_output_loss: 0.0267 - category_output_accuracy: 0.9821 - color_output_accuracy: 0.9921 - val_loss: 0.1869 - val_category_output_loss: 0.1561 - val_color_output_loss: 0.0292 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9901\n",
            "Epoch 18/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0737 - category_output_loss: 0.0433 - color_output_loss: 0.0304 - category_output_accuracy: 0.9846 - color_output_accuracy: 0.9901 - val_loss: 0.2534 - val_category_output_loss: 0.1358 - val_color_output_loss: 0.1189 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9604\n",
            "Epoch 19/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0356 - category_output_loss: 0.0201 - color_output_loss: 0.0155 - category_output_accuracy: 0.9945 - color_output_accuracy: 0.9960 - val_loss: 0.2340 - val_category_output_loss: 0.1593 - val_color_output_loss: 0.0757 - val_category_output_accuracy: 0.9545 - val_color_output_accuracy: 0.9723\n",
            "Epoch 20/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0468 - category_output_loss: 0.0283 - color_output_loss: 0.0185 - category_output_accuracy: 0.9891 - color_output_accuracy: 0.9926 - val_loss: 0.2282 - val_category_output_loss: 0.1986 - val_color_output_loss: 0.0295 - val_category_output_accuracy: 0.9505 - val_color_output_accuracy: 0.9921\n",
            "Epoch 21/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0550 - category_output_loss: 0.0313 - color_output_loss: 0.0237 - category_output_accuracy: 0.9896 - color_output_accuracy: 0.9916 - val_loss: 0.2192 - val_category_output_loss: 0.1725 - val_color_output_loss: 0.0445 - val_category_output_accuracy: 0.9564 - val_color_output_accuracy: 0.9842\n",
            "Epoch 22/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0290 - category_output_loss: 0.0186 - color_output_loss: 0.0105 - category_output_accuracy: 0.9921 - color_output_accuracy: 0.9955 - val_loss: 0.2545 - val_category_output_loss: 0.2240 - val_color_output_loss: 0.0296 - val_category_output_accuracy: 0.9446 - val_color_output_accuracy: 0.9941\n",
            "Epoch 23/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0411 - category_output_loss: 0.0177 - color_output_loss: 0.0234 - category_output_accuracy: 0.9940 - color_output_accuracy: 0.9921 - val_loss: 0.2389 - val_category_output_loss: 0.1903 - val_color_output_loss: 0.0479 - val_category_output_accuracy: 0.9525 - val_color_output_accuracy: 0.9842\n",
            "Epoch 24/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0416 - category_output_loss: 0.0155 - color_output_loss: 0.0261 - category_output_accuracy: 0.9945 - color_output_accuracy: 0.9916 - val_loss: 0.2346 - val_category_output_loss: 0.1965 - val_color_output_loss: 0.0389 - val_category_output_accuracy: 0.9525 - val_color_output_accuracy: 0.9842\n",
            "Epoch 25/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0307 - category_output_loss: 0.0133 - color_output_loss: 0.0174 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9931 - val_loss: 0.2624 - val_category_output_loss: 0.1994 - val_color_output_loss: 0.0614 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9842\n",
            "Epoch 26/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0327 - category_output_loss: 0.0203 - color_output_loss: 0.0124 - category_output_accuracy: 0.9940 - color_output_accuracy: 0.9950 - val_loss: 0.1982 - val_category_output_loss: 0.1652 - val_color_output_loss: 0.0304 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9921\n",
            "Epoch 27/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0330 - category_output_loss: 0.0177 - color_output_loss: 0.0153 - category_output_accuracy: 0.9950 - color_output_accuracy: 0.9950 - val_loss: 0.1921 - val_category_output_loss: 0.1426 - val_color_output_loss: 0.0475 - val_category_output_accuracy: 0.9683 - val_color_output_accuracy: 0.9861\n",
            "Epoch 28/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0417 - category_output_loss: 0.0191 - color_output_loss: 0.0227 - category_output_accuracy: 0.9911 - color_output_accuracy: 0.9931 - val_loss: 0.3609 - val_category_output_loss: 0.3237 - val_color_output_loss: 0.0344 - val_category_output_accuracy: 0.9188 - val_color_output_accuracy: 0.9921\n",
            "Epoch 29/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0335 - category_output_loss: 0.0206 - color_output_loss: 0.0129 - category_output_accuracy: 0.9940 - color_output_accuracy: 0.9940 - val_loss: 0.2949 - val_category_output_loss: 0.2119 - val_color_output_loss: 0.0809 - val_category_output_accuracy: 0.9545 - val_color_output_accuracy: 0.9822\n",
            "Epoch 30/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0296 - category_output_loss: 0.0184 - color_output_loss: 0.0111 - category_output_accuracy: 0.9950 - color_output_accuracy: 0.9955 - val_loss: 1.0213 - val_category_output_loss: 0.9447 - val_color_output_loss: 0.0703 - val_category_output_accuracy: 0.7644 - val_color_output_accuracy: 0.9822\n",
            "Epoch 31/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0647 - category_output_loss: 0.0563 - color_output_loss: 0.0084 - category_output_accuracy: 0.9792 - color_output_accuracy: 0.9980 - val_loss: 1.7845 - val_category_output_loss: 1.7401 - val_color_output_loss: 0.0400 - val_category_output_accuracy: 0.7089 - val_color_output_accuracy: 0.9901\n",
            "Epoch 32/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0435 - category_output_loss: 0.0314 - color_output_loss: 0.0121 - category_output_accuracy: 0.9891 - color_output_accuracy: 0.9950 - val_loss: 0.3152 - val_category_output_loss: 0.2317 - val_color_output_loss: 0.0794 - val_category_output_accuracy: 0.9505 - val_color_output_accuracy: 0.9743\n",
            "Epoch 33/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0337 - category_output_loss: 0.0237 - color_output_loss: 0.0100 - category_output_accuracy: 0.9911 - color_output_accuracy: 0.9960 - val_loss: 0.2794 - val_category_output_loss: 0.1453 - val_color_output_loss: 0.1310 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9663\n",
            "Epoch 34/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0268 - category_output_loss: 0.0091 - color_output_loss: 0.0176 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9940 - val_loss: 0.5563 - val_category_output_loss: 0.1453 - val_color_output_loss: 0.4137 - val_category_output_accuracy: 0.9703 - val_color_output_accuracy: 0.9030\n",
            "Epoch 35/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0465 - category_output_loss: 0.0101 - color_output_loss: 0.0364 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9891 - val_loss: 0.1764 - val_category_output_loss: 0.1314 - val_color_output_loss: 0.0434 - val_category_output_accuracy: 0.9762 - val_color_output_accuracy: 0.9881\n",
            "Epoch 36/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0208 - category_output_loss: 0.0071 - color_output_loss: 0.0137 - category_output_accuracy: 0.9985 - color_output_accuracy: 0.9940 - val_loss: 0.2979 - val_category_output_loss: 0.1943 - val_color_output_loss: 0.0996 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9723\n",
            "Epoch 37/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0298 - category_output_loss: 0.0130 - color_output_loss: 0.0169 - category_output_accuracy: 0.9955 - color_output_accuracy: 0.9950 - val_loss: 0.2408 - val_category_output_loss: 0.1761 - val_color_output_loss: 0.0616 - val_category_output_accuracy: 0.9604 - val_color_output_accuracy: 0.9861\n",
            "Epoch 38/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0320 - category_output_loss: 0.0185 - color_output_loss: 0.0135 - category_output_accuracy: 0.9931 - color_output_accuracy: 0.9955 - val_loss: 0.2592 - val_category_output_loss: 0.1921 - val_color_output_loss: 0.0649 - val_category_output_accuracy: 0.9525 - val_color_output_accuracy: 0.9822\n",
            "Epoch 39/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0315 - category_output_loss: 0.0179 - color_output_loss: 0.0135 - category_output_accuracy: 0.9940 - color_output_accuracy: 0.9960 - val_loss: 0.2553 - val_category_output_loss: 0.2054 - val_color_output_loss: 0.0489 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9842\n",
            "Epoch 40/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0225 - category_output_loss: 0.0153 - color_output_loss: 0.0072 - category_output_accuracy: 0.9940 - color_output_accuracy: 0.9965 - val_loss: 0.2504 - val_category_output_loss: 0.2243 - val_color_output_loss: 0.0260 - val_category_output_accuracy: 0.9545 - val_color_output_accuracy: 0.9921\n",
            "Epoch 41/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0206 - category_output_loss: 0.0136 - color_output_loss: 0.0069 - category_output_accuracy: 0.9955 - color_output_accuracy: 0.9970 - val_loss: 0.2817 - val_category_output_loss: 0.2097 - val_color_output_loss: 0.0689 - val_category_output_accuracy: 0.9465 - val_color_output_accuracy: 0.9822\n",
            "Epoch 42/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0312 - category_output_loss: 0.0114 - color_output_loss: 0.0198 - category_output_accuracy: 0.9975 - color_output_accuracy: 0.9931 - val_loss: 0.2485 - val_category_output_loss: 0.1857 - val_color_output_loss: 0.0623 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9782\n",
            "Epoch 43/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0313 - category_output_loss: 0.0096 - color_output_loss: 0.0217 - category_output_accuracy: 0.9955 - color_output_accuracy: 0.9921 - val_loss: 0.1948 - val_category_output_loss: 0.1313 - val_color_output_loss: 0.0615 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9802\n",
            "Epoch 44/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0234 - category_output_loss: 0.0126 - color_output_loss: 0.0108 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9960 - val_loss: 0.2683 - val_category_output_loss: 0.1578 - val_color_output_loss: 0.1076 - val_category_output_accuracy: 0.9723 - val_color_output_accuracy: 0.9703\n",
            "Epoch 45/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0293 - category_output_loss: 0.0213 - color_output_loss: 0.0079 - category_output_accuracy: 0.9931 - color_output_accuracy: 0.9975 - val_loss: 1.3368 - val_category_output_loss: 1.2360 - val_color_output_loss: 0.0975 - val_category_output_accuracy: 0.6931 - val_color_output_accuracy: 0.9624\n",
            "Epoch 46/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0440 - category_output_loss: 0.0317 - color_output_loss: 0.0124 - category_output_accuracy: 0.9886 - color_output_accuracy: 0.9965 - val_loss: 0.2757 - val_category_output_loss: 0.2482 - val_color_output_loss: 0.0281 - val_category_output_accuracy: 0.9485 - val_color_output_accuracy: 0.9901\n",
            "Epoch 47/50\n",
            "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0335 - category_output_loss: 0.0199 - color_output_loss: 0.0135 - category_output_accuracy: 0.9936 - color_output_accuracy: 0.9945 - val_loss: 0.3290 - val_category_output_loss: 0.2066 - val_color_output_loss: 0.1188 - val_category_output_accuracy: 0.9564 - val_color_output_accuracy: 0.9723\n",
            "Epoch 48/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0184 - category_output_loss: 0.0108 - color_output_loss: 0.0076 - category_output_accuracy: 0.9980 - color_output_accuracy: 0.9975 - val_loss: 0.3429 - val_category_output_loss: 0.1729 - val_color_output_loss: 0.1672 - val_category_output_accuracy: 0.9604 - val_color_output_accuracy: 0.9604\n",
            "Epoch 49/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0370 - category_output_loss: 0.0123 - color_output_loss: 0.0247 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9931 - val_loss: 0.2331 - val_category_output_loss: 0.1768 - val_color_output_loss: 0.0568 - val_category_output_accuracy: 0.9604 - val_color_output_accuracy: 0.9861\n",
            "Epoch 50/50\n",
            "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0439 - category_output_loss: 0.0137 - color_output_loss: 0.0302 - category_output_accuracy: 0.9945 - color_output_accuracy: 0.9916 - val_loss: 0.1924 - val_category_output_loss: 0.1571 - val_color_output_loss: 0.0328 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9921\n",
            "[INFO] serializing network...\n",
            "[INFO] serializing category label binarizer...\n",
            "[INFO] serializing color label binarizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahui3f0kopQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the total loss, category loss, and color loss\n",
        "lossNames = [\"loss\", \"category_output_loss\", \"color_output_loss\"]\n",
        "plt.style.use(\"ggplot\")\n",
        "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
        "\n",
        "# loop over the loss names\n",
        "for (i, l) in enumerate(lossNames):\n",
        "\t# plot the loss for both the training and validation data\n",
        "\ttitle = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
        "\tax[i].set_title(title)\n",
        "\tax[i].set_xlabel(\"Epoch #\")\n",
        "\tax[i].set_ylabel(\"Loss\")\n",
        "\tax[i].plot(np.arange(0, EPOCHS), H.history[l], label=l)\n",
        "\tax[i].plot(np.arange(0, EPOCHS), H.history[\"val_\" + l],\n",
        "\t\tlabel=\"val_\" + l)\n",
        "\tax[i].legend()\n",
        "\n",
        "# save the losses figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# plt.savefig(\"{}_losses.png\".format(args[\"plot\"]))\n",
        "plt.close()\n",
        "\n",
        "# create a new figure for the accuracies\n",
        "accuracyNames = [\"category_output_accuracy\", \"color_output_accuracy\"]\n",
        "plt.style.use(\"ggplot\")\n",
        "(fig, ax) = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "# loop over the accuracy names\n",
        "for (i, l) in enumerate(accuracyNames):\n",
        "\t# plot the loss for both the training and validation data\n",
        "\tax[i].set_title(\"Accuracy for {}\".format(l))\n",
        "\tax[i].set_xlabel(\"Epoch #\")\n",
        "\tax[i].set_ylabel(\"Accuracy\")\n",
        "\tax[i].plot(np.arange(0, EPOCHS), H.history[l], label=l)\n",
        "\tax[i].plot(np.arange(0, EPOCHS), H.history[\"val_\" + l],\n",
        "\t\tlabel=\"val_\" + l)\n",
        "\tax[i].legend()\n",
        "\n",
        "# save the accuracies figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# plt.savefig(\"{}_accs.png\".format(args[\"plot\"]))\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv23dD2r6C5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAhyikkR3_IK",
        "colab_type": "code",
        "outputId": "6dc99cc5-c425-4b80-888c-8e8d41abd3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# USAGE\n",
        "# python classify.py --model output/fashion.model \\\n",
        "#\t--categorybin output/category_lb.pickle --colorbin output/color_lb.pickle \\\n",
        "#\t--image examples/black_dress.jpg\n",
        "\n",
        "\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
        "# \thelp=\"path to trained model model\")\n",
        "# ap.add_argument(\"-l\", \"--categorybin\", required=True,\n",
        "# \thelp=\"path to output category label binarizer\")\n",
        "# ap.add_argument(\"-c\", \"--colorbin\", required=True,\n",
        "# \thelp=\"path to output color label binarizer\")\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "\n",
        "\n",
        "args = { \"model\":\"multi-output-classification/output/fashion.model\", \n",
        "        \"categorybin\":\"multi-output-classification/output/category_lb.pickle\", \n",
        "        \"colorbin\":\"multi-output-classification/output/color_lb.pickle\", \n",
        "        \"image\":\"multi-output-classification/examples/black_dress.jpg\" }\n",
        "\n",
        "# load the image\n",
        "image = cv2.imread(args[\"image\"])\n",
        "output = imutils.resize(image, width=400)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# pre-process the image for classification\n",
        "image = cv2.resize(image, (96, 96))\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# load the trained convolutional neural network from disk, followed\n",
        "# by the category and color label binarizers, respectively\n",
        "print(\"[INFO] loading network...\")\n",
        "model = load_model(args[\"model\"], custom_objects={\"tf\": tf})\n",
        "categoryLB = pickle.loads(open(args[\"categorybin\"], \"rb\").read())\n",
        "colorLB = pickle.loads(open(args[\"colorbin\"], \"rb\").read())\n",
        "\n",
        "# classify the input image using Keras' multi-output functionality\n",
        "print(\"[INFO] classifying image...\")\n",
        "(categoryProba, colorProba) = model.predict(image)\n",
        "\n",
        "plt.imshow(image.reshape(IMAGE_DIMS))\n",
        "plt.show()\n",
        "categoryIdx = categoryProba[0].argmax()\n",
        "colorIdx = colorProba[0].argmax()\n",
        "categoryLabel = categoryLB.classes_[categoryIdx]\n",
        "colorLabel = colorLB.classes_[colorIdx]\n",
        "\n",
        "# draw the category label and color label on the image\n",
        "categoryText = \"category: {} ({:.2f}%)\".format(categoryLabel,\n",
        "\tcategoryProba[0][categoryIdx] * 100)\n",
        "colorText = \"color: {} ({:.2f}%)\".format(colorLabel,\n",
        "\tcolorProba[0][colorIdx] * 100)\n",
        "print(categoryText)\n",
        "print(colorText)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "[INFO] classifying image...\n",
            "category: shirt (93.84%)\n",
            "color: black (99.11%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db6TqmSu3_Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZezI4Oc3-_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnD-BaXu2nFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}